{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sourjyamukherjee/Projects/Confluence_Retriever/Confluence_Retriever/processing_scraped_docs_consolidated_script.py:39: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-base-en-v1.5\",\n",
      "/home/sourjyamukherjee/Projects/Confluence_Retriever/Confluence_Retriever/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/sourjyamukherjee/Projects/Confluence_Retriever/Confluence_Retriever/processing_scraped_docs_consolidated_script.py:54: LangChainDeprecationWarning: The class `AzureChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import AzureChatOpenAI``.\n",
      "  multimodal_llm = AzureChatOpenAI(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langsmith import Client \n",
    "from langsmith import traceable\n",
    "import base64\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_community.chat_models import AzureChatOpenAI\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from typing import List, Dict\n",
    "import json\n",
    "import re\n",
    "import asyncio\n",
    "from tqdm.asyncio import tqdm\n",
    "import torch\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.retrievers.ensemble import EnsembleRetriever\n",
    "from processing_scraped_docs_consolidated_script import vectordb_path,collection_name\n",
    "\n",
    "torch.classes.__path__ = []\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv() \n",
    "\n",
    "# Environment Variables\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "# os.environ['LANGCHAIN_TRACING'] = 'true'\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"Confluence Retriever\"\n",
    "# os.environ[\"LANGCHAIN_API_KEY\"] = str(os.environ.get(\"LANGCHAIN_API_KEY\"))\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = str(os.environ.get(\"LANGCHAIN_API_KEY\"))\n",
    "os.environ[\"LANGSMITH_ENDPOINT\"]=\"https://api.smith.langchain.com\"\n",
    "# os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "\n",
    "client = Client()\n",
    "\n",
    "AZURE_OPENAI_VERSION=os.environ.get(\"AZURE_OPENAI_VERSION\")\n",
    "AZURE_OPENAI_DEPLOYMENT=os.environ.get(\"AZURE_OPENAI_DEPLOYMENT\")\n",
    "AZURE_OPENAI_ENDPOINT=os.environ.get(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_OPENAI_KEY=os.environ.get(\"AZURE_OPENAI_KEY\")\n",
    "AZURE_OPENAI_DEPLOYMENT_GPT4=os.environ.get(\"AZURE_OPENAI_DEPLOYMENT_GPT4\")\n",
    "\n",
    "gpt_35 = AzureChatOpenAI(\n",
    "    openai_api_version=AZURE_OPENAI_VERSION,\n",
    "    azure_deployment=AZURE_OPENAI_DEPLOYMENT,\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_key=AZURE_OPENAI_KEY,\n",
    "    temperature=0.75,\n",
    "    )\n",
    "\n",
    "gpt_4o = AzureChatOpenAI(\n",
    "    openai_api_version=AZURE_OPENAI_VERSION,\n",
    "    azure_deployment=\"gpt-4o\",\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_key=AZURE_OPENAI_KEY,\n",
    "    temperature=0.6,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_72207/2639565893.py:2: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectordb = Chroma(persist_directory=vectordb_path, collection_name=collection_name, embedding_function=embedding_model)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'page_content'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# all_child_docs = vectordb._collection.get()\u001b[39;00m\n\u001b[1;32m      4\u001b[0m all_child_docs \u001b[38;5;241m=\u001b[39m vectordb\u001b[38;5;241m.\u001b[39mget()\n\u001b[0;32m----> 5\u001b[0m bm25_retriever \u001b[38;5;241m=\u001b[39m \u001b[43mBM25Retriever\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_child_docs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/Confluence_Retriever/Confluence_Retriever/venv/lib/python3.10/site-packages/langchain_community/retrievers/bm25.py:99\u001b[0m, in \u001b[0;36mBM25Retriever.from_documents\u001b[0;34m(cls, documents, bm25_params, preprocess_func, **kwargs)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfrom_documents\u001b[39m(\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m     87\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BM25Retriever:\n\u001b[1;32m     88\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;124;03m    Create a BM25Retriever from a list of Documents.\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;124;03m        A BM25Retriever instance.\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 99\u001b[0m     texts, metadatas, ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpage_content\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_texts(\n\u001b[1;32m    103\u001b[0m         texts\u001b[38;5;241m=\u001b[39mtexts,\n\u001b[1;32m    104\u001b[0m         bm25_params\u001b[38;5;241m=\u001b[39mbm25_params,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    109\u001b[0m     )\n",
      "File \u001b[0;32m~/Projects/Confluence_Retriever/Confluence_Retriever/venv/lib/python3.10/site-packages/langchain_community/retrievers/bm25.py:100\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfrom_documents\u001b[39m(\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m     87\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BM25Retriever:\n\u001b[1;32m     88\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;124;03m    Create a BM25Retriever from a list of Documents.\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;124;03m        A BM25Retriever instance.\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m     texts, metadatas, ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\n\u001b[0;32m--> 100\u001b[0m         \u001b[38;5;241m*\u001b[39m((\u001b[43md\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpage_content\u001b[49m, d\u001b[38;5;241m.\u001b[39mmetadata, d\u001b[38;5;241m.\u001b[39mid) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m documents)\n\u001b[1;32m    101\u001b[0m     )\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_texts(\n\u001b[1;32m    103\u001b[0m         texts\u001b[38;5;241m=\u001b[39mtexts,\n\u001b[1;32m    104\u001b[0m         bm25_params\u001b[38;5;241m=\u001b[39mbm25_params,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    109\u001b[0m     )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'page_content'"
     ]
    }
   ],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-base-en-v1.5\", model_kwargs={\"device\": \"cpu\"})\n",
    "vectordb = Chroma(persist_directory=vectordb_path, collection_name=collection_name, embedding_function=embedding_model)\n",
    "# all_child_docs = vectordb._collection.get()\n",
    "all_child_docs = vectordb.get()\n",
    "bm25_retriever = BM25Retriever.from_documents(all_child_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_child_docs = vectordb._collection.get(include=[ \"documents\",\"metadatas\"],limit=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "def convert_chroma_response_to_docs(chroma_response):\n",
    "    \"\"\"\n",
    "    Converts Chroma's get() method response into a list of LangChain Document objects.\n",
    "    \n",
    "    Args:\n",
    "        chroma_response (dict): The dictionary response from ChromaDB's get() method.\n",
    "    \n",
    "    Returns:\n",
    "        List[Document]: A list of Document objects with `page_content` and `metadata`.\n",
    "    \"\"\"\n",
    "    documents = chroma_response.get(\"documents\", [])\n",
    "    metadatas = chroma_response.get(\"metadatas\", [])\n",
    "\n",
    "    # Ensure both lists are of the same length\n",
    "    if len(documents) != len(metadatas):\n",
    "        raise ValueError(\"Mismatch between number of documents and metadata entries.\")\n",
    "\n",
    "    return [Document(page_content=doc, metadata=meta) for doc, meta in zip(documents, metadatas)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ids', 'embeddings', 'documents', 'uris', 'data', 'metadatas', 'included'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_child_docs = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_child_docs=convert_chroma_response_to_docs(all_child_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'attachments': \"['- ./md_output/Sponsor  Ads/sponsorads-sequence diagram.png', '- ./md_output/Sponsor  Ads/sponsorads-Class diagram.jpg', '- ./md_output/Sponsor  Ads/sponsorads-sequence diagram.jpg', '- ./md_output/Sponsor  Ads/Class diagram.jpg']\", 'source': 'https://bigbasket.atlassian.net/wiki/rest/api/content/719290385', 'title': 'Sponsor  Ads'}, page_content='| Bigbasket APP | CRUD for Sponsor ads Registration Existing business logic for banner ads to support sponsor ads in all the flows like ,  Create Update  Bulk Download report, Page to show success and failure report with links to download View View All Validation position upload Date range upload | Create/update - build support of ad create/update for promotion type - add promotion type in ad position and date ranges tables - top l3 categories check, check for keyword is product is belongs to top l3 categories - banner ad check  - SKU id/product id information - Add can set up for parent or child product - save promotion type in search ad, position, date range and add filter in fetching banner ads or promotion ads View: - pass additional info and fetch based on promotion type Download  \\xa0- send email to login user with report Validation - production id validation - promotion type enum check   * list slug input validation * DC and PD id validation, is pd id present in DC or not * check all validations in spec | 9 |\\n|  | Update to ES | Additional fields in ES promotion\\\\_type, product\\\\_id, ad\\\\_class | 3 |\\n|  | Product Listing page | IN Product Listing Page  Calls ES to fetch sponsor ads Business logic to exclude sponsor ads product ids from SOLR and fetch list to show Merging parent and child products if parent/child has sponsor ads then shown it for entire group.  bubble up\\xa0– sponsor ad in parent child group  Merging logic of products, product id is always shown at registered location excluding stock out product Additional fields in response [ad class]  --  get parents for sponsor ads  exclude them via SOLR search  merge in the list  get parent child combination and add to SKU list  --  For stock out item, add to log | 6 |\\n|  | Search banner ads API | Exclude sponsor ads while fetching from ES | 2 |\\n|  | Email Notification | For download report, failure and success report\\xa0 will send email. | 2 |\\n|  | Kafka | Present flow : In Same thread ES is updated  Need to build system for both banner ads and sponsor ads for async Kafka producer and consumer | 2 |\\n|  | Cron | enhance cron to support for sponsor ads expiry update in ES | 1 |'),\n",
       " Document(metadata={'attachments': \"['- ./md_output/Sponsor  Ads/sponsorads-sequence diagram.png', '- ./md_output/Sponsor  Ads/sponsorads-Class diagram.jpg', '- ./md_output/Sponsor  Ads/sponsorads-sequence diagram.jpg', '- ./md_output/Sponsor  Ads/Class diagram.jpg']\", 'source': 'https://bigbasket.atlassian.net/wiki/rest/api/content/719290385', 'title': 'Sponsor  Ads'}, page_content='## Sequence Diagram :\\n\\nFor bulk upload\\n\\n\\xa0 \\xa0From Admin ui --------------------------------- Big basket\\xa0\\xa0[ in big basket app read the file from UI, and load all data to dictionary and call an async method with will upload to DB and then update to ES]\\n\\nin Async method :\\n\\n\\xa0 \\xa0- update to DB\\n\\n\\xa0 - update ES\\n\\nWhich is long thread in case of bulk upload\\n\\n## Class Diagram:\\n\\nEPICs :\\n\\nEpic 1:\\n\\n\\xa0 \\xa0 \\xa0Search Ad Admin UI for CRUD operations\\n\\n\\xa0 \\xa0 \\xa0Search Date range Admin UI and CRUD operations\\n\\n\\xa0 \\xa0 Position, search ad class\\n\\nEpic 2 :\\n\\n\\xa0 \\xa0 \\xa0Update ES with sponsor ads information\\n\\n\\xa0 \\xa0 \\xa0product listing API\\n\\n\\xa0 \\xa0 \\xa0Banner Ads API\\n\\nEpic 3:\\n\\n\\xa0 \\xa0 \\xa0 Cron\\n\\n\\xa0 \\xa0 \\xa0 Email notification\\n\\n\\xa0 \\xa0 \\xa0 Sold search ads admin UI and API with Download report\\n\\n\\xa0 \\xa0 \\xa0 Agg term, update weightage in solr\\n\\n\\xa0 \\xa0\\xa0 Bulk CRUD operations for search Ad\\n\\n# Coding\\n\\n## Tables:\\n\\n| sl no | Table | Extra fields | Sql query | comments |\\n| --- | --- | --- | --- | --- |\\n| 1 | saul\\\\_adbuyer | NA |  |  |\\n| 2 | saul\\\\_searchad | list\\\\_type,\\xa0ad\\\\_class,\\xa0product\\\\_id,\\xa0promotion\\\\_type | alter table saul\\\\_searchad add list\\\\_type varchar(30); alter table saul\\\\_searchad add adclass\\\\_id int(6) alter table saul\\\\_searchad add product\\\\_id int(11) alter table saul\\\\_searchad add promotion\\\\_type varchar(30); |  |\\n| 3 | saul\\\\_adposition | promotion\\\\_type, list\\\\_type | alter table saul\\\\_adposition add list\\\\_type varchar(30); alter table saul\\\\_adposition add promotion\\\\_type varchar(30); | promotion\\\\_type : banner/sponsor |\\n| 4 | saul\\\\_searchaddaterange | promotion\\\\_type | alter table saul\\\\_searchaddaterange add promotion\\\\_type varchar(30); |  |\\n| 5 | saul\\\\_sarchad\\\\_keyword\\\\_list | NA |  |  |\\n| 6 | saul\\\\_searchad\\\\_dcs | NA |  |  |\\n| 7 | saul\\\\_adclass | id,name, url\\\\_path | create table adclass ( id int(11) NOT NULL AUTO\\\\_INCREMENT PRIMARY KEY, name varchar(20) NOT NULL,  url\\\\_path varchar(200) NOT NULL, created\\\\_on datetime(6) NOT NULL, updated\\\\_on datetime(6) NOT NULL); | Table for adclass |\\n\\nES :\\n\\nAdditional fields in ES,\\xa0 promotion\\\\_type, product\\\\_id, ad\\\\_class\\n\\n## Task break up :\\n\\n|  |  |  |  |\\n| --- | --- | --- | --- |\\n| Application | Tasks | Sub Task | Effort |\\n| App | Product new version API integration in APP to display sponsor ads along with products |  |  |\\n| DB | New column additions in following tables |  |  |\\n| Elastic Search | New key additions in Es search banners ads document |  |  |'),\n",
       " Document(metadata={'attachments': \"['- ./md_output/Sponsor  Ads/sponsorads-sequence diagram.png', '- ./md_output/Sponsor  Ads/sponsorads-Class diagram.jpg', '- ./md_output/Sponsor  Ads/sponsorads-sequence diagram.jpg', '- ./md_output/Sponsor  Ads/Class diagram.jpg']\", 'source': 'https://bigbasket.atlassian.net/wiki/rest/api/content/719290385', 'title': 'Sponsor  Ads'}, page_content='# \\n\\n# Introduction\\n\\nSponsor ads is a product promotion service.\\n\\nProduct spec :[https://do](https://docs.google.com/document/d/10_dpNxYOuFg8-ZTFYQMOwnKPpasbSFjxlm79AWska_E/edit)[cs.google.com/document/d/10\\\\_dpNxYOuFg8-ZTFYQMOwnKPpasbSFjxlm79AWska\\\\_E/edit](https://docs.google.com/document/d/10_dpNxYOuFg8-ZTFYQMOwnKPpasbSFjxlm79AWska_E/edit)\\n\\nThe sponsor ads service is implemented along with search banner ads system. The search banners ads is also promotions based service.\\n\\nThe sponsor ads service feature is implemented along with search banners Ads system.\\n\\nThe search banners Ads system\\xa0 has sponsor ads as well. From ES based on requirements sponsor/search banners are fetched and served to clients .\\n\\nFor product listing page, fetch sponsor ads and merge with list. The sponsor ads are registered in system via Django Admin Ui.\\n\\nSearch banners ads :\\xa0\\n\\n# **Sponsor Ads How Document :**\\n\\nSponsor ads is product promoting service.\\xa0To sponsor ads, ad is required to be registered in the system. The Products ads registration\\xa0is accepted via Admin UI.\\n\\nFrom Admin UI\\n\\nUser can create an ad for a particular product for a particular position and in specific date range.\\n\\nThe sponsor ads feature is implemented in\\xa0 search banner Ads features, same\\xa0 tables are used with some additional fields like product id, list type, product id, promotion\\\\_type and ad\\\\_class.\\n\\nSponsor ads required ad buyer, to set up sponsor ads for a product.\\n\\nPrerequisite for sponsor ad set up, following are required for sponsor ad, please populate data \\xa0if not present.\\n\\nAd buyer creation\\n\\nPositions\\n\\nDate ranges\\n\\nDevice Types\\n\\nTo set up sponsor ad for a product\\n\\n1. Following items need to choose on create page in Admin UI\\n2. position\\n3. date range\\n4. device type[android, ios, web]\\n5. DCs\\n6. List type (l1, l2, l3, pb, pbpc, sis and search)\\n7. list slug (categories of list type or keywords)\\n8. Ad class (sponsor, recommendation, trending) is label to show on sponsor ads.\\n9. On Save, the system will validate position , date range and other validations and create sponsor ad.\\n\\nBulk upload is supported for create and deactivate.\\n\\nSponsor ad can be deactivate/activate/edit from admin UI.\\n\\n**Display:**\\n\\nProduct ads are displayed in product listing page in mobile app.\\n\\nThe product ad is displayed for the following flows Search/browse pages on registered position based on availability.\\n\\n# Design\\n'),\n",
       " Document(metadata={'attachments': \"['- ./md_output/Sponsor  Ads/sponsorads-sequence diagram.png', '- ./md_output/Sponsor  Ads/sponsorads-Class diagram.jpg', '- ./md_output/Sponsor  Ads/sponsorads-sequence diagram.jpg', '- ./md_output/Sponsor  Ads/Class diagram.jpg']\", 'source': 'https://bigbasket.atlassian.net/wiki/rest/api/content/719290385', 'title': 'Sponsor  Ads'}, page_content='| Django Admin UI | Add new fields in UI  Download load link to download success and failure report  UI Pages (Django Admin UI) View All View Create Update Download report Bulk upload create Bulk upload deactivate | For UI :  List-slug (keyword): multiple keywords are allowed  Keywords are multiple  Drop down for list type    Search Ad :  \\xa0 \\xa0create  \\xa0 \\xa0update  \\xa0 \\xa0view  \\xa0 \\xa0search  export\\\\_data  \\xa0 \\xa0delete  Search date range :  \\xa0 \\xa0create  \\xa0 \\xa0migrate(export\\\\_data)  Bulk create/edit upload :  \\xa0 \\xa0 \\xa0 \\xa0- add  \\xa0 \\xa0 \\xa0 \\xa0- update  \\xa0 \\xa0 \\xa0 \\xa0- delete  sold search ads:  \\xa0 \\xa0 \\xa0 - view  \\xa0 \\xa0 \\xa0 - search  \\xa0 \\xa0 \\xa0 - email CSV | 5 |\\n| Cart service | Cart service, pass through search ad id information from APP to Kafka |  |  |\\n| Bigbasket APP | Weekly Cron read from Aerospike sponsor ad bucket\\xa0 and update configured weightage. |  | 3 |\\n\\n## API contracts:\\n\\nProduct list and next API are new versions\\n\\n### Product listing API\\n\\n<https://www.bigbasket.com/mapi/v3.5.1/product/list/?referrer=ps.h&user_query=biscuit&suggest_term=biscuit&type=ps&search_type=h&slug=biscuit>&extra\\\\_params={%22sp\\\\_inject%22:true}\\n\\nAdditional fields search\\\\_ad\\\\_id and ad\\\\_class\\\\_name\\n\\nad\\\\_class\\\\_name # label is used for\\xa0 display , the value is\\xa0passed at parent level.\\n\\nsearch\\\\_ad\\\\_id # search\\\\_ad id is passed to kafka to improve agg terms and snowplow metrics\\n\\nNo change in version, the changes will be done in latest version.\\n\\nQuery param added in request :\\xa0extra\\\\_params={%22sp\\\\_inject%22:true}\\n\\nResponse format\\n\\njavaDJangoProduct list API responsetruetrue{\\n\"status\": 0,\\n\"message\": \"success\",\\n\"response\": {\\n\"base\\\\_img\\\\_url\": \"https://www.bigbasket.com/media/uploads/p/ml/\",\\n\"cart\\\\_info\": {},\\n\"tab\\\\_info\": [{\\n\"filtered\\\\_on\": [],\\n\"category\\\\_index\": 0,\\n\"tab\\\\_name\": \"Maggie\",\\n\"correction\": null,\\n\"tab\\\\_info\": [],\\n\"related\\\\_search\": {},\\n\"tab\\\\_type\": \"all\",\\n\"sort\\\\_opts\": [],\\n\"categories\": [],\\n\"header\\\\_sel\": 0,\\n\"screen\\\\_name\": \"maggie\",\\n\"sorted\\\\_on\": \"relevance\",\\n\"filter\\\\_opts\": [],\\n\"q\": \"maggie\",\\n\"product\\\\_info\": {\\n\"tot\\\\_pages\": 2,\\n\"p\\\\_count\": 49,\\n\"products\": [{\\n\"llc\\\\_n\": \"Instant Noodles\",\\n\"pop\\\\_rank\": 0,\\n\"all\\\\_prods\": [],\\n\"is\\\\_new\": false,\\n\"pack\\\\_desc\": \"\",\\n\"p\\\\_type\": \"veg\",\\n\"attrs\": {},\\n\"llc\\\\_s\": \"type=pc&slug=instant-noodles\",\\n\"store\\\\_availability\": [{\\n\"tab\\\\_type\": \"standard\",\\n\"pstat\": \"A\",\\n\"availability\\\\_info\\\\_id\": \"1.1\",\\n\"store\\\\_id\": \"1\"\\n},\\n{\\n\"tab\\\\_type\": \"standard\",\\n\"pstat\": \"A\",\\n\"availability\\\\_info\\\\_id\": \"6655.1\",'),\n",
       " Document(metadata={'attachments': \"['- ./md_output/Sponsor  Ads/sponsorads-sequence diagram.png', '- ./md_output/Sponsor  Ads/sponsorads-Class diagram.jpg', '- ./md_output/Sponsor  Ads/sponsorads-sequence diagram.jpg', '- ./md_output/Sponsor  Ads/Class diagram.jpg']\", 'parent_id': '7991612-5', 'source': 'https://bigbasket.atlassian.net/wiki/rest/api/content/719290385', 'title': 'Sponsor  Ads'}, page_content='### Title: Sponsor  Ads\\n\"store\\\\_id\": \"6655\"\\n},\\n{\\n\"tab\\\\_type\": \"standard\",\\n\"pstat\": \"O\",\\n\"availability\\\\_info\\\\_id\": \"3630.1\",\\n\"store\\\\_id\": \"3630\"\\n},\\n{\\n\"tab\\\\_type\": \"express\",\\n\"pstat\": \"A\",\\n\"availability\\\\_info\\\\_id\": \"8117.57\",\\n\"store\\\\_id\": \"8117\"\\n}\\n],\\n\"sku\": 30007113,\\n\"dis\\\\_t\": \"P\",\\n\"tlc\\\\_s\": \"snacks-branded-foods\",\\n\"p\\\\_img\\\\_url\": \"30007113\\\\_9-maggi-oats-noodles-masala.jpg\",\\n\"p\\\\_brand\": \"MAGGI \",\\n\"store\\\\_ids\": [],\\n\"es\\\\_percent\": 0,\\n\"tlc\\\\_n\": \"Snacks & Branded Foods\",\\n\"p\\\\_promo\\\\_info\": {},\\n\"gift\\\\_msg\": \"\",\\n\"combo\\\\_skus\": [],\\n\"offer\\\\_score\": 0,\\n\"dis\\\\_val\": \"10\",\\n\"sale\\\\_info\": {},\\n\"display\\\\_order\": 0,\\n\"sp\": \"88.00\",\\n\"mrp\": \"98.00\",\\n\"es\\\\_amt\": 0,\\n\"w\": \"300 g\",\\n\"pc\\\\_n\": \"Instant Noodles\",\\n\"p\\\\_desc\": \"Oats Noodles - Masala\",\\n\"sponsored\\\\_product\\\\_info\": {\\n\"ad\\\\_id\": 12321,\\n\"ad\\\\_label\": \"trending\",')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_child_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_retriever = vectordb.as_retriever(search_type=\"similarity\", search_kwargs={'k': 5})\n",
    "docs = dense_retriever.invoke(\"planogram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'attachments': '[]', 'parent_id': 80944137.0, 'source': 'https://bigbasket.atlassian.net/wiki/rest/api/content/719454209', 'title': 'Planogram MoM and Q&A'}, page_content='### Title: Planogram MoM and Q&A\\n# Q&A'),\n",
       " Document(metadata={'attachments': '[]', 'parent_id': 80944137.0, 'source': 'https://bigbasket.atlassian.net/wiki/rest/api/content/719454209', 'title': 'Planogram MoM and Q&A'}, page_content=\"### Title: Planogram MoM and Q&A\\n9. Two different API's are provided by BBAdmin; one for bulk picking and another for packed sku's. In packed sku's api, planogram need to get the parent sku and return the location for picking.\\n10. In CC, picking always happens in crates to the nearest roundoff of the requested weight/quantity in pcs.\\n11. In CC, crates are stacked in planogram only via receiving.\\n    1. Enhancement: Support a random crate with a barcode label to enter into planogram via TI operation.\\n12. In DC, F&V are received in crates and stacked in crates in secondary location.\\n13. In DC, F&V will be in packs in picking location.\\n14. In DC, F&V SKU's which are cratable will always be in crates in picking location.\\n15. Empty bin→Closet to the existing picking loc→ closest to the secondary.\"),\n",
       " Document(metadata={'attachments': \"['- ./md_output/3 Pager - Dynamic Location/image2019-12-11_15-23-47.png', '- ./md_output/3 Pager - Dynamic Location/image2019-12-11_15-23-6.png', '- ./md_output/3 Pager - Dynamic Location/image2019-12-11_14-40-10.png', '- ./md_output/3 Pager - Dynamic Location/image2019-12-11_13-3-17.png', '- ./md_output/3 Pager - Dynamic Location/image2019-12-11_12-52-41.png', '- ./md_output/3 Pager - Dynamic Location/image2019-12-11_12-46-41.png', '- ./md_output/3 Pager - Dynamic Location/image2019-12-10_13-55-18.png', '- ./md_output/3 Pager - Dynamic Location/image2019-12-10_13-54-34.png', '- ./md_output/3 Pager - Dynamic Location/image2019-12-10_13-9-34.png', '- ./md_output/3 Pager - Dynamic Location/image2019-12-10_12-53-15.png', '- ./md_output/3 Pager - Dynamic Location/image2019-12-5_14-9-18.png', '- ./md_output/3 Pager - Dynamic Location/image2019-12-5_14-8-16.png', '- ./md_output/3 Pager - Dynamic Location/image2019-12-5_14-6-26.png', '- ./md_output/3 Pager - Dynamic Location/image2019-12-5_11-35-48.png', '- ./md_output/3 Pager - Dynamic Location/Global Max SKU Bin COnfig.jpg', '- ./md_output/3 Pager - Dynamic Location/IMG_20190826_131734.jpg', '- ./md_output/3 Pager - Dynamic Location/IMG_20190826_131810.jpg', '- ./md_output/3 Pager - Dynamic Location/IMG_20190822_164449.jpg', '- ./md_output/3 Pager - Dynamic Location/IMG_20190822_170527.jpg', '- ./md_output/3 Pager - Dynamic Location/IMG_20190822_162823.jpg', '- ./md_output/3 Pager - Dynamic Location/IMG_20190822_161632.jpg', '- ./md_output/3 Pager - Dynamic Location/IMG_20190822_154038.jpg', '- ./md_output/3 Pager - Dynamic Location/Rule engine 6 for Dynamic Locations V1-Soft+Slow Moving+Heavy_Not Heavy.jpg', '- ./md_output/3 Pager - Dynamic Location/Rule engine 5 for Dynamic Locations V1-Soft+Fast Moving+Heavy_Not Heavy.jpg', '- ./md_output/3 Pager - Dynamic Location/Rule engine 4 for Dynamic Locations V1-Fragile+Slow Moving+Heavy_Not Heavy.jpg', '- ./md_output/3 Pager - Dynamic Location/Rule engine 3 for Dynamic Locations V2-Fragile+Fast Moving+Heavy_Not Heavy.jpg', '- ./md_output/3 Pager - Dynamic Location/Rule engine 2 for Dynamic Locations V3-Hard+Slow Moving+Heavy_Not Heavy.jpg', '- ./md_output/3 Pager - Dynamic Location/Rule engine 1 for Dynamic Locations V3-Hard+Fast Moving+Heavy_Not Heavy.jpg', '- ./md_output/3 Pager - Dynamic Location/Stacking Group Creation and Global COnfig.jpg', '- ./md_output/3 Pager - Dynamic Location/Global Planogram-Rack & Shelf.jpg', '- ./md_output/3 Pager - Dynamic Location/Replenishment Global Config.jpg', '- ./md_output/3 Pager - Dynamic Location/Replenishment Local Config.jpg', '- ./md_output/3 Pager - Dynamic Location/image2019-5-24_0-3-13.png', '- ./md_output/3 Pager - Dynamic Location/image2019-5-20_9-23-58.png', '- ./md_output/3 Pager - Dynamic Location/image2019-5-20_9-23-31.png', '- ./md_output/3 Pager - Dynamic Location/image2019-5-20_9-23-8.png', '- ./md_output/3 Pager - Dynamic Location/image2019-5-20_9-22-41.png', '- ./md_output/3 Pager - Dynamic Location/image2019-5-20_9-22-22.png', '- ./md_output/3 Pager - Dynamic Location/image2019-5-20_9-21-55.png', '- ./md_output/3 Pager - Dynamic Location/image2019-5-17_11-25-58.png', '- ./md_output/3 Pager - Dynamic Location/image2019-5-17_11-22-57.png', '- ./md_output/3 Pager - Dynamic Location/image2019-5-17_8-24-30.png', '- ./md_output/3 Pager - Dynamic Location/image2019-5-17_8-22-9.png', '- ./md_output/3 Pager - Dynamic Location/image2019-5-17_7-48-51.png', '- ./md_output/3 Pager - Dynamic Location/image2019-5-17_7-43-1.png', '- ./md_output/3 Pager - Dynamic Location/image2019-5-17_7-36-47.png', '- ./md_output/3 Pager - Dynamic Location/image2019-5-17_7-36-2.png', '- ./md_output/3 Pager - Dynamic Location/image2019-5-17_7-35-11.png', '- ./md_output/3 Pager - Dynamic Location/image2019-5-16_10-25-6.png', '- ./md_output/3 Pager - Dynamic Location/image2019-5-16_10-21-39.png']\", 'source': 'https://bigbasket.atlassian.net/wiki/rest/api/content/704053344', 'title': '3 Pager - Dynamic Location'}, page_content='| Upper/Lower/Medium Bins | Planogram | Database/Upload |  |\\n| Pick Subtype | Planogram | Database/Upload |  |\\n| Picking/Secondary | Planogram | Database/Upload |  |\\n| Max Sku\\'s | Planogram | Database/Upload |  |\\n| Bin Type | Planogram | Database/Upload |  |\\n| Aisle | Planogram | Database/Upload |  |\\n| Rack | Planogram | Database/Upload |  |\\n| Shelf | Planogram | Database/Upload |  |\\n\\n## 3. Aisle Sequencing\\n\\n| Aisle Name | Sequence | Comments |\\n| --- | --- | --- |\\n| Z | 1 |  |\\n| AB | 2 |  |\\n| A | 3 |  |\\n\\n# G. Data Flow\\n\\n## 1. DC Data Flow\\n\\n### i. F&V Data Flow\\n\\n| Process\\\\Interactions | App-Eretail | Eretail-Planogram | App-Planogram | Eretail-Monolith | Monolith-Planogram | Planogram-Monolith | Planogram-App | Planogram-Eretail | Planogram Action |\\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\\n| GRN FlowThrough | [{skuId, farmerId, eretailCode, weight, quantity, unitKg, unitPc,\\xa0timestamp, isContainer}] | [{skuId, farmerId, eretailCode, weight, quantity, unitKg, unitPc,\\xa0timestamp, isContainer}] |  |  |  |  |  |  | Fetch Batch |\\n| One Click/Auto GRN | NA |  |  | {grnId, InterdcId} | [{skuId, farmerId, eretailCode, weight, quantity, unitKg, unitPc,\\xa0timestamp, isContainer}] |  |  |  | Fetch Batch |\\n| Stacking | ***Post***:\\xa0{skuId, farmerId, eretailCode, weight, quantity, unitKg, unitPc, timestamp, location, quantity, isContainer} |  | ***Get Location***: {skuId, farmerId, eretailCode, weight, quantity, unitKg, unitPc,\\xa0timestamp, isContainer} | ***Post***:\\xa0{skuId, farmerId, eretailCode, weight, quantity, unitKg, unitPc, timestamp, location, quantity, isContainer} | ***Post***:\\xa0{skuId, farmerId, eretailCode, weight, quantity, unitKg, unitPc, timestamp, location, quantity, isContainer} | ***Post***: Success Response. | ***Get Location Response:*** [{skuId, farmerId, eretailCode, weight, quantity, unitKg, unitPc, timestamp, location, quantity, isContainer, location}] |  | ***Get operation:*** Planogram returns the list of locations where to stack.  ***Post Operation:*** Planogram updates state in the database to \"Stacked\". |'),\n",
       " Document(metadata={'attachments': '[]', 'parent_id': 80944137.0, 'source': 'https://bigbasket.atlassian.net/wiki/rest/api/content/719454209', 'title': 'Planogram MoM and Q&A'}, page_content='### Title: Planogram MoM and Q&A\\n# Release Plan for F&V\\n\\n1. Move the location module(stacking, picking, replenishment, write-off) from Eretail to BB Admin.\\n2. Batch Creation and recommending locations based on batches.\\n   1. getting the batch information.\\n   2. tracking the flow of quantity from cc->dc->ds->customer.\\n3. dynamic location identification based on 3d bin packing.\\n   1. stacking.\\n   2. picking\\n   3. replinshment'),\n",
       " Document(metadata={'attachments': '[]', 'parent_id': '93557187', 'source': 'https://bigbasket.atlassian.net/wiki/rest/api/content/729415879', 'title': 'Collection center 3 pager (`HOW` part)'}, page_content='### Title: Collection center 3 pager (`HOW` part)\\n* Whenever stacking needs to happen for sku, stacker scans the sku and gets the location to stack from planogram service. Once stacked at location, ack will be passed to planogram\\n\\n## How:\\xa0Barcode Generation Logic\\n\\n* Packaging app will send the bar code of source and then new destination crate barcode. Planogram stores this mapping\\n* Packing app api signature:\\n\\n## How: Packing/Picking Logic (eRetail)\\n\\n* Once the indent are raised in CC, it starts to flow to CC eretail via api:\\n* If the indent sku is not present in inventory, eretail will initiate packing of this sku first\\n* In packing app, packer first gets the parent sku (or non-sellable sku) to the packing table, starts packing the indent sku, once done, will post the packing details via api:')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.2/29.2 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.19.0 in ./venv/lib/python3.10/site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.10/site-packages (from spacy) (59.6.0)\n",
      "Collecting wasabi<1.2.0,>=0.9.1\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.0/183.0 KB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in ./venv/lib/python3.10/site-packages (from spacy) (2.10.6)\n",
      "Collecting weasel<0.5.0,>=0.1.0\n",
      "  Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.3/50.3 KB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (204 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.8/204.8 KB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting thinc<8.4.0,>=8.3.4\n",
      "  Downloading thinc-8.3.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typer<1.0.0,>=0.3.0 in ./venv/lib/python3.10/site-packages (from spacy) (0.15.2)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.10/site-packages (from spacy) (3.1.6)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.9-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (156 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.9/156.9 KB\u001b[0m \u001b[31m931.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in ./venv/lib/python3.10/site-packages (from spacy) (4.67.1)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in ./venv/lib/python3.10/site-packages (from spacy) (24.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in ./venv/lib/python3.10/site-packages (from spacy) (2.32.3)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.12-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.3/124.3 KB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting language-data>=1.2\n",
      "  Downloading language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.12.2 in ./venv/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in ./venv/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./venv/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
      "Collecting blis<1.3.0,>=1.2.0\n",
      "  Downloading blis-1.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting confection<1.0.0,>=0.0.1\n",
      "  Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Requirement already satisfied: rich>=10.11.0 in ./venv/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
      "Requirement already satisfied: click>=8.0.0 in ./venv/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in ./venv/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0\n",
      "  Downloading cloudpathlib-0.21.0-py3-none-any.whl (52 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.7/52.7 KB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting smart-open<8.0.0,>=5.2.1\n",
      "  Downloading smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.7/61.7 KB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.10/site-packages (from jinja2->spacy) (3.0.2)\n",
      "Collecting marisa-trie>=1.1.0\n",
      "  Downloading marisa_trie-1.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m673.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: markdown-it-py>=2.2.0 in ./venv/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./venv/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
      "Requirement already satisfied: wrapt in ./venv/lib/python3.10/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./venv/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Installing collected packages: cymem, wasabi, spacy-loggers, spacy-legacy, smart-open, murmurhash, marisa-trie, cloudpathlib, catalogue, blis, srsly, preshed, language-data, langcodes, confection, weasel, thinc, spacy\n",
      "Successfully installed blis-1.2.0 catalogue-2.0.10 cloudpathlib-0.21.0 confection-0.1.5 cymem-2.0.11 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.2.1 murmurhash-1.0.12 preshed-3.0.9 smart-open-7.1.0 spacy-3.8.4 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 thinc-8.3.4 wasabi-1.1.3 weasel-0.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Load spaCy model - you'll need to install it with: python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_keywords(query, method=\"hybrid\"):\n",
    "    \"\"\"\n",
    "    Extract meaningful keywords from a query using multiple techniques.\n",
    "    \n",
    "    Args:\n",
    "        query: The user query string\n",
    "        method: Extraction method - \"spacy\", \"tfidf\", or \"hybrid\" (default)\n",
    "    \n",
    "    Returns:\n",
    "        List of extracted keywords\n",
    "    \"\"\"\n",
    "    # Clean the query\n",
    "    clean_query = re.sub(r'[^\\w\\s]', ' ', query.lower())\n",
    "    \n",
    "    if method == \"spacy\":\n",
    "        # Use spaCy for linguistic-based extraction\n",
    "        doc = nlp(clean_query)\n",
    "        \n",
    "        # Extract nouns, proper nouns, and technical terms\n",
    "        keywords = []\n",
    "        for token in doc:\n",
    "            # Include nouns and proper nouns\n",
    "            if token.pos_ in [\"NOUN\", \"PROPN\"]:\n",
    "                keywords.append(token.text)\n",
    "            \n",
    "            # Include verbs that might be technical actions\n",
    "            if token.pos_ == \"VERB\" and len(token.text) > 3:  # Avoid common short verbs\n",
    "                keywords.append(token.text)\n",
    "                \n",
    "        # Extract named entities\n",
    "        for ent in doc.ents:\n",
    "            keywords.append(ent.text)\n",
    "            \n",
    "        # Extract noun chunks (noun phrases)\n",
    "        for chunk in doc.noun_chunks:\n",
    "            keywords.append(chunk.text)\n",
    "            \n",
    "    elif method == \"tfidf\":\n",
    "        # Use TF-IDF for statistical keyword extraction\n",
    "        # This works better with a corpus, but we can use a simple version\n",
    "        vectorizer = TfidfVectorizer(\n",
    "            max_df=0.9, \n",
    "            min_df=1,\n",
    "            stop_words='english',\n",
    "            use_idf=True\n",
    "        )\n",
    "        \n",
    "        # Create a small corpus with the query\n",
    "        corpus = [clean_query]\n",
    "        \n",
    "        # Add some general text to help with IDF calculations\n",
    "        corpus.extend([\n",
    "            \"password credentials login authentication\",\n",
    "            \"configuration settings setup installation\",\n",
    "            \"database server network connection\",\n",
    "            \"user account profile settings\"\n",
    "        ])\n",
    "        \n",
    "        # Fit and transform\n",
    "        tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "        \n",
    "        # Get feature names\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "        \n",
    "        # Get scores for the query (first document)\n",
    "        scores = tfidf_matrix[0].toarray()[0]\n",
    "        \n",
    "        # Get top scoring terms\n",
    "        top_indices = scores.argsort()[-10:][::-1]  # Get top 10 terms\n",
    "        keywords = [feature_names[i] for i in top_indices]\n",
    "        \n",
    "    else:  # hybrid approach\n",
    "        # Combine both methods\n",
    "        spacy_keywords = extract_keywords(query, \"spacy\")\n",
    "        tfidf_keywords = extract_keywords(query, \"tfidf\")\n",
    "        \n",
    "        # Combine and count occurrences\n",
    "        all_keywords = spacy_keywords + tfidf_keywords\n",
    "        keyword_counts = Counter(all_keywords)\n",
    "        \n",
    "        # Get keywords that appear in both methods or have high counts\n",
    "        keywords = [k for k, c in keyword_counts.items() if c > 1]\n",
    "        \n",
    "        # Add any technical terms that might have been missed\n",
    "        technical_terms = extract_technical_terms(query)\n",
    "        keywords.extend(technical_terms)\n",
    "        \n",
    "        # If we don't have enough keywords, add top terms from either method\n",
    "        if len(keywords) < 3:\n",
    "            remaining = set(all_keywords) - set(keywords)\n",
    "            keywords.extend(list(remaining)[:5])\n",
    "    \n",
    "    # Remove duplicates and normalize\n",
    "    unique_keywords = []\n",
    "    seen = set()\n",
    "    for keyword in keywords:\n",
    "        normalized = keyword.lower().strip()\n",
    "        if normalized and normalized not in seen and len(normalized) > 2:\n",
    "            seen.add(normalized)\n",
    "            unique_keywords.append(normalized)\n",
    "    \n",
    "    return unique_keywords\n",
    "\n",
    "def extract_technical_terms(query):\n",
    "    \"\"\"Extract technical terms that might be missed by other methods\"\"\"\n",
    "    technical_patterns = [\n",
    "        r'password[s]?',\n",
    "        r'credential[s]?',\n",
    "        r'token[s]?',\n",
    "        r'api[_\\s]?key[s]?',\n",
    "        r'secret[s]?',\n",
    "        r'config(?:uration)?[s]?',\n",
    "        r'database[s]?|db[s]?',\n",
    "        r'server[s]?',\n",
    "        r'endpoint[s]?',\n",
    "        r'url[s]?',\n",
    "        r'[a-zA-Z0-9_]+\\.(?:py|js|java|cpp|rb|go|rs|php|html|css|json|yaml|yml|xml|md|txt)'  # file extensions\n",
    "    ]\n",
    "    \n",
    "    technical_terms = []\n",
    "    for pattern in technical_patterns:\n",
    "        matches = re.finditer(pattern, query, re.IGNORECASE)\n",
    "        for match in matches:\n",
    "            technical_terms.append(match.group(0))\n",
    "    \n",
    "    return technical_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tell', 'stacking', 'process', 'bigbasket', 'planogram']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"give me the password and credentils for newrelic and grafana dashboards\"\n",
    "query = \"give me steps for blocking a url at jdvar gateway\"\n",
    "query = \"tell me about the stacking process at bigbasket planogram\"\n",
    "extract_keywords(query,\"hybrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"child_chunks_400\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
